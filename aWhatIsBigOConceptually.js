// ==========
// Question:
// ==========
// What is Big O Notation?

// Google Search Perspective:
// Big O notation is used in Computer Science to describe the performance or complexity of an algorithm. 
// Big O specifically describes the worst-case scenario, and can be used to describe the execution time 
// required or the space used (e.g. in memory or on disk) by an algorithm.

// Software Engineer Perspective:
// It allows us to formally talk about how the runtime of an algorithm grows as the input grows. It’s 
// a way of describing the relationship between the input to a function as it grows, and how that changes
//  the runtime of that function. The relationship between the input size and the time elapsed relative to 
// that input. We don’t care too much more than that.

// Wikipedia Perspective:
// Big O notation is a mathematical notation that describes the limiting behavior of a function when the argument 
// tends towards a particular value or infinity

// ==========
// Question:
// ==========
// Where did Big O Notation come from?
// It is a member of a family of notations invented by Paul Bachmann,[1] Edmund Landau,[2] and others, 
// collectively called Bachmann–Landau notation or asymptotic notation.
